# -*- coding: utf-8 -*-
"""project banking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dxDz9Rkyd_gqNJtlvkyeqmkFewTFCBHU
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import spacy

data = pd.read_excel("june_sent.xlsx")
data.head(5)



data.columns = ['Name','Address1','Address2',
                     'Address3','City','State','Zipcode','PhoneNumber','PrimaryEmailAddress']

x = data.iloc[:,[0,1]]

x.head(10)

import nltk
nltk.download("all")

"""**Data Cleaning**"""

## Preprocess

import string 
punctuation = string.punctuation

from nltk.corpus import stopwords
stopwords = stopwords.words("english")

def _clean(Name):
  # convert all the text in Lower case
  Name = Name.lower()
  # Remove punchuation
  Name = "".join(x for x in Name if x not in punctuation )

  # Remove stop words
  words = Name.split()
  words = [w for w in words if w not in stopwords]

  text = " ".join(words)
  return Name

_clean("This is a sample text!!!")  # This is a stop word so its removed.

data['cleaned'] = data['Name'].apply(_clean)

data[['Name', 'cleaned']]

"""**Check Maximum Keywords are used in Data**"""

## Maximum Keywords are used in Data

from collections import Counter
complete_text = " ".join(data['cleaned'])
words = complete_text.split()
Counter(words).most_common(50)

Name = data.Name.unique()

for index, tweet in enumerate(data["Name"][0:15]):
    print(index+1,".",tweet)

pos = nltk.pos_tag(data['Name'])
pos

chunks = nltk.ne_chunk(pos, binary=True)
for chunk in chunks:
  print(chunk)

"""**Print the organization Name or Person Name**"""

# Print top organization Name or Person Name

nlp = spacy.load("en_core_web_sm")
docs = nlp.pipe(Name)
for doc in docs:

  for ent in doc.ents:

    print(ent.text, ent.label_)

























